{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shitote/go_crud/blob/main/Skimlit_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rk1fleklkZtc"
      },
      "source": [
        "# SkimLit\n",
        "\n",
        "This is an NLP model to make reading medical abstracts easier.\n",
        "\n",
        "The paper replicated and the source of the dataset is at https://arxiv.or/abs/171006071 and https://arxiv.org/abs/1612.05251"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtL9YgxSs2Le"
      },
      "outputs": [],
      "source": [
        "!invidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfVTIoxns6ya"
      },
      "source": [
        "## Get data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz6lbNLxtxOi"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/Franck-Dernoncourt/pubmed-rct\n",
        "!ls pubmed-rct"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8S8of_EMuGXT"
      },
      "outputs": [],
      "source": [
        "# Check what files are in the pulmed 20k dataset\n",
        "!ls pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "eY46BKgfvCP1"
      },
      "outputs": [],
      "source": [
        "# Start experimenting with numbers replaced by @ sing\n",
        "data_dir = \"/content/pubmed-rct/PubMed_20k_RCT_numbers_replaced_with_at_sign/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yEDQC-OLuQYo"
      },
      "outputs": [],
      "source": [
        "# Check all the filenames in the target directory\n",
        "import os\n",
        "filenames = [data_dir + filenames for filenames in os.listdir(data_dir)]\n",
        "filenames"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTMFC6dou1Xk"
      },
      "source": [
        "## Preprocess data\n",
        "\n",
        "become one with the data (visualize, visualize, visualize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "bv92OWGjvj0X"
      },
      "outputs": [],
      "source": [
        "# Create a function to read and write the lines of a document.\n",
        "def get_lines(filename):\n",
        "  \"\"\"\n",
        "  Read filenames (a text filename) and returns the lines of text as a list.\n",
        "\n",
        "  Args:\n",
        "    filename: a strin containing the target filpath.\n",
        "\n",
        "  returns:\n",
        "    A list of strings with one string per line from the target filename.\n",
        "  \"\"\"\n",
        "  with open(filename, 'r') as f:\n",
        "    return f.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzNlLwRhw2kl"
      },
      "outputs": [],
      "source": [
        "# Read in the train lines\n",
        "train_lines = get_lines(data_dir+\"train.txt\")\n",
        "train_lines[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x6d1_ZsxHrd"
      },
      "outputs": [],
      "source": [
        "len(train_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3oNyyaNLxfDc"
      },
      "source": [
        "##### Think about the format of the data.\n",
        "\n",
        "How the data should be represented.\n",
        "\n",
        "```\n",
        "`[{'line_number': 0,\n",
        "   'target': 'BACKGROUND',\n",
        "   'text': 'Emotional eating is associated with overeatin and the development of obasity .\\n',\n",
        "   'totoal lines': 11},\n",
        "  -----]\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIv6uRselwp1"
      },
      "source": [
        "Function to create the dataset to be in the same format as above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "dlIqsBO7mJln"
      },
      "outputs": [],
      "source": [
        "def preprocess_text_with_line_numbers(filename):\n",
        "  \"\"\"\n",
        "  Returns a list of directories of abstract line data.\n",
        "\n",
        "  Takes in dilname, reads it's contents and sorts throuh each line,\n",
        "  extractin thins like the taret label, the text of the sentense,\n",
        "  how many sentences are in the current abstract and what sentence\n",
        "  number the target line is.\n",
        "  \"\"\"\n",
        "  input_lines = get_lines(filename)\n",
        "  abstract_lines = \"\"\n",
        "  abstract_samples = []\n",
        "\n",
        "  for line in input_lines:\n",
        "    if line.startswith(\"###\"):\n",
        "      abstract_id = line\n",
        "      abstract_lines = \"\"\n",
        "    elif line.isspace():\n",
        "      abstract_line_split = abstract_lines.splitlines()\n",
        "\n",
        "      for abstract_line_number, abstract_lines in enumerate(abstract_line_split):\n",
        "        line_data = {}\n",
        "        target_text_split = abstract_lines.split(\"\\t\")\n",
        "        line_data['target'] = target_text_split[0]\n",
        "        line_data['text'] = target_text_split[1].lower()\n",
        "        line_data['line_number'] = abstract_line_number\n",
        "        line_data['total_lines'] = len(abstract_line_split) - 1\n",
        "        abstract_samples.append(line_data)\n",
        "    else:\n",
        "      abstract_lines += line\n",
        "\n",
        "  return abstract_samples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZoEpdiwwuwx7"
      },
      "outputs": [],
      "source": [
        "# Get data from file and preprocess it\n",
        "%%time\n",
        "train_samples = preprocess_text_with_line_numbers(data_dir + \"train.txt\")\n",
        "val_samples = preprocess_text_with_line_numbers(data_dir + 'dev.txt')\n",
        "test_samples = preprocess_text_with_line_numbers(data_dir + 'test.txt')\n",
        "print(len(train_samples), len(val_samples), len(test_samples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QlJxDFDNvhd-"
      },
      "outputs": [],
      "source": [
        "# Check the abstract of the training data\n",
        "train_samples[:20]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_Hg_qfHv8Q8"
      },
      "source": [
        "Now that the data is in the form a dict, it is easy to turn it into a dataframe."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WdhkMC1Gzqvp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "train_df = pd.DataFrame(train_samples)\n",
        "val_df = pd.DataFrame(val_samples)\n",
        "test_df = pd.DataFrame(test_samples)\n",
        "train_df.head(14)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kWMtfeJo0H9N"
      },
      "outputs": [],
      "source": [
        "# Find the distributions of labels.\n",
        "train_df.target.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcxDb1zA1Wqx"
      },
      "outputs": [],
      "source": [
        "# Check the langth of different lines.\n",
        "train_df.total_lines.plot.hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNX0DoUeXTVb"
      },
      "source": [
        "### Get list of sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oz6CFprzXl9o"
      },
      "outputs": [],
      "source": [
        "# Convert abstract text lines into lists\n",
        "train_sentences = train_df[\"text\"].tolist()\n",
        "val_sentences = val_df['text'].tolist()\n",
        "test_sentences = test_df['text'].tolist()\n",
        "len(train_sentences), len(val_sentences), len(test_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjiNQSggYgg4"
      },
      "outputs": [],
      "source": [
        "train_sentences[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V6Idun6pYoPG"
      },
      "source": [
        "### Make numerical labels (ML models require numeric labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jYNzq4paKJ_"
      },
      "outputs": [],
      "source": [
        "# One hot encode labels\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "one_hot_encoder = OneHotEncoder(sparse=False)\n",
        "train_labels_one_hot = one_hot_encoder.fit_transform(train_df['target'].to_numpy().reshape(-1, 1))\n",
        "val_labels_one_hot = one_hot_encoder.transform(val_df['target'].to_numpy().reshape(-1, 1))\n",
        "test_labels_one_hot = one_hot_encoder.transform(test_df['target'].to_numpy().reshape(-1, 1))\n",
        "\n",
        "# Check what one hot encoder labels look like.\n",
        "train_labels_one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSAwqUD-ahyH"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "print(sklearn.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OQjqGVkam6k"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.constant(train_labels_one_hot)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIPvk-8_cT0s"
      },
      "source": [
        "### Label encode labels\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GQauce-odgfW"
      },
      "outputs": [],
      "source": [
        "# Extract labels (\"Target\" columns) and encode them into integers\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels_encoded = label_encoder.fit_transform(train_df['target'].to_numpy())\n",
        "val_labels_encoded = label_encoder.transform(val_df['target'].to_numpy())\n",
        "test_labels_encoded = label_encoder.transform(test_df['target'].to_numpy())\n",
        "\n",
        "train_labels_encoded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q267V4dcf-g1"
      },
      "outputs": [],
      "source": [
        "# Get class names and number of classes form LabelEncoder instance\n",
        "num_classes = len(label_encoder.classes_)\n",
        "class_names = label_encoder.classes_\n",
        "num_classes, class_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAXReWtIg0P1"
      },
      "source": [
        "## Startin a series of modellling eperiments...\n",
        "\n",
        "Try Different models and see what works the best to the problem at hand.\n",
        "\n",
        " start with the baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1z_I-HxXiyAj"
      },
      "source": [
        "## Model 0: ettin a baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4V5Rmiyvi3_p"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create a pipeline.\n",
        "model_0 = Pipeline([\n",
        "    ('tf-idf', TfidfVectorizer()),\n",
        "    ('clf', MultinomialNB())\n",
        "])\n",
        "\n",
        "# Fit the pipeline to the training data.\n",
        "model_0.fit(X=train_sentences,\n",
        "            y=train_labels_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49bkqUHbpZEJ",
        "outputId": "90dd7dfb-2951-4671-ab52-14e147febadd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7218323844829869"
            ]
          },
          "metadata": {},
          "execution_count": 158
        }
      ],
      "source": [
        "model_0.score(X=val_sentences,\n",
        "              y=val_labels_encoded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nS6YCz6bqGFX"
      },
      "outputs": [],
      "source": [
        "# Make predictions using the baseline model.\n",
        "baseline_preds = model_0.predict(val_sentences)\n",
        "baseline_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBSLxT_Gqgwe"
      },
      "outputs": [],
      "source": [
        "val_labels_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G-v_Z9ZIqmIH"
      },
      "source": [
        "#### Dowload the helper functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQMe9bKArtBj"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "YTtE4YE2rwzv"
      },
      "outputs": [],
      "source": [
        "from helper_functions import calculate_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z-McIRR8r-kL"
      },
      "outputs": [],
      "source": [
        "# Calculate baseline results\n",
        "baseline_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                     y_pred=baseline_preds)\n",
        "baseline_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgdsZ36qsUVT"
      },
      "source": [
        "### Preparing the data (that is the texts) for deep seqeunce models\n",
        "\n",
        "Before any deep learning modelin you need to create vectorization and embedding layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "id": "Ewe0fEJqtwSR"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fVFNDxit8aq"
      },
      "outputs": [],
      "source": [
        "# How long is each sentence on average?\n",
        "sent_lens = [len(sentence.split()) for sentence in train_sentences]\n",
        "avg_sent_len = np.mean(sent_lens)\n",
        "avg_sent_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZA38qbdu_-z"
      },
      "outputs": [],
      "source": [
        "# What's the distribution look like?\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(sent_lens, bins=25);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzp2h_FmzQY-"
      },
      "outputs": [],
      "source": [
        "# How long of a sentence length covers 95% os examples?\n",
        "output_seq_len = int(np.percentile(sent_lens, 95))\n",
        "output_seq_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5opVYUp00VO5"
      },
      "outputs": [],
      "source": [
        "# Find the maximum length of the sentences\n",
        "max(sent_lens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "re4Lh5bz0rAP"
      },
      "source": [
        "### Create text vectorizer layer\n",
        "\n",
        "Make a layer which maps texts from words to numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "uh1C0nzy3WoC"
      },
      "outputs": [],
      "source": [
        "max_tokens = 68000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 169,
      "metadata": {
        "id": "2QRkpZ5b-Wje"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 170,
      "metadata": {
        "id": "NkZXb8pR3d8C"
      },
      "outputs": [],
      "source": [
        "# Create a text vectorizer\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "# Use the default TextVectorization parameters.\n",
        "text_vectorizer = TextVectorization(max_tokens=max_tokens,\n",
        "                                    output_sequence_length=output_seq_len\n",
        "                                    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 171,
      "metadata": {
        "id": "L5k6AYzp4S7N"
      },
      "outputs": [],
      "source": [
        "text_vectorizer.adapt(train_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2bACWxq2v4DB"
      },
      "outputs": [],
      "source": [
        "# Tes tht text vectorizer on random sentences\n",
        "import random\n",
        "target_sentence = random.choice(train_sentences)\n",
        "print(f'Text: \\n {target_sentence}')\n",
        "print(f'\\nLength of text: {len(target_sentence.split())}')\n",
        "print(f'\\nVectorized text: {text_vectorizer([target_sentence])}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QaGGwVsPxHFW"
      },
      "outputs": [],
      "source": [
        " # Number of words in the training vocabulary\n",
        " rct_20k_text_vocab = text_vectorizer.get_vocabulary()\n",
        " print(f\"Number of words in vocab: {len(rct_20k_text_vocab)}\")\n",
        " print(f'Most common words in the vocab: {rct_20k_text_vocab[:5]}')\n",
        " print(f'Least common words in the vocab: {rct_20k_text_vocab[-5:]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1f5B4ktnyz5c"
      },
      "outputs": [],
      "source": [
        "# Get the configuration of the text vectorizer\n",
        "text_vectorizer.get_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGS6WMx3zQfs"
      },
      "source": [
        "### Create Custom Text Embeddin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "id": "8SqZhoozz_d2"
      },
      "outputs": [],
      "source": [
        "# Create token embeddin layer\n",
        "token_embed = layers.Embedding(input_dim=len(rct_20k_text_vocab),\n",
        "                                   output_dim=128,\n",
        "                                   mask_zero=True,\n",
        "                                   name=\"token_embedding\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K_OQKsvr2A_T"
      },
      "outputs": [],
      "source": [
        "# Example of a sentence that have undergone embedding.\n",
        "print(f'Sentence before vectorization: \\n {target_sentence}')\n",
        "vectorized_sentence = text_vectorizer([target_sentence])\n",
        "print(f\"The vectorized text: /n {vectorized_sentence}\")\n",
        "embedded_sentence = token_embed(vectorized_sentence)\n",
        "print(f'The embedded sentence:\\n\\n {embedded_sentence}')\n",
        "print(f'Embedding sentence shape: {embedded_sentence.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ml05kflN2wGu"
      },
      "source": [
        "## Creating dataset (Make sure the data loads as fastr a possible)\n",
        "\n",
        "Use: tf.data API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whqNwyUfL53A"
      },
      "outputs": [],
      "source": [
        "# Turn the data into tensorflow Datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels_one_hot))\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels_one_hot))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_sentences, test_labels_one_hot))\n",
        "\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLhUy453NtIO"
      },
      "outputs": [],
      "source": [
        "# Take the TensorSliceDataset's and turn them into prefected datasets\n",
        "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "valid_dataset = valid_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_dataset = test_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CquZE7LBPIjF"
      },
      "source": [
        "## Model 1: Conv1D with token embeddingsm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 179,
      "metadata": {
        "id": "kCkebDcwVPWC"
      },
      "outputs": [],
      "source": [
        "# Create 1D conv model\n",
        "inputs = layers.Input(shape=(1,), dtype=tf.string)\n",
        "text_vectors = text_vectorizer(inputs)\n",
        "token_embeddings = token_embed(text_vectors)\n",
        "x = layers.Conv1D(64, kernel_size=5, padding='same', activation='relu')(token_embeddings)\n",
        "x = layers.GlobalAveragePooling1D()(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "model_1 = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model_1.compile(loss='categorical_crossentropy',\n",
        "                optimizer = tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQz1HCk4XNiY"
      },
      "outputs": [],
      "source": [
        "model_1.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I6GgsUcpZBPb"
      },
      "outputs": [],
      "source": [
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y7Zc8opKZo2O"
      },
      "outputs": [],
      "source": [
        "# Fit model_1\n",
        "history_model_1 = model_1.fit(train_dataset,\n",
        "                              steps_per_epoch=int(0.1*len(train_dataset)),\n",
        "                              epochs=3,\n",
        "                              validation_data=valid_dataset,\n",
        "                              validation_steps=int(0.1*len(valid_dataset))  # Validaton on onnly 10% of the datches.\n",
        "                              )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkkWnqEibg-I"
      },
      "outputs": [],
      "source": [
        "# Make Evaluation on the whole validation dataset\n",
        "model_1.evaluate(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XvbxGqjDNVXZ"
      },
      "outputs": [],
      "source": [
        "# Make prediction (The model displays a prediction probability to each class.)\n",
        "model_1_pred_probs = model_1.predict(valid_dataset)\n",
        "model_1_pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "krKMPWtmN735"
      },
      "outputs": [],
      "source": [
        "# Conver the prediction probabilities to classes\n",
        "# Argmax return the position in the array where there is a high prediction probability.\n",
        "model_1_preds = tf.argmax(model_1_pred_probs, axis=1)\n",
        "model_1_preds[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlSCao92PBJM"
      },
      "outputs": [],
      "source": [
        "class_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J-bK2HxEPZqN"
      },
      "outputs": [],
      "source": [
        "# Calculate model_1 results\n",
        "model_1_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                    y_pred=model_1_preds)\n",
        "model_1_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZFvQ-f7QJYv"
      },
      "source": [
        " ## Model 2: Feature extraction with pretrained token embeddings\n",
        "\n",
        " The use of universal sentence encoder.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {
        "id": "1y0IN_tPU-cx"
      },
      "outputs": [],
      "source": [
        "# Download pretrained TensorFlow Hub use\n",
        "import tensorflow_hub as hub\n",
        "model_url = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "tf_hub_embedding_layer = hub.KerasLayer(model_url,\n",
        "                                        trainable=False,\n",
        "                                        name=\"Universal_sentence_encoder\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBJdWKi8V_AS"
      },
      "outputs": [],
      "source": [
        "# TRest the pretrained embedding on a sentence.\n",
        "random_train_sentence = random.choice(train_sentences)\n",
        "print(f\"Random sentences:\\n {random_train_sentence}\")\n",
        "use_embedding_sentence = tf_hub_embedding_layer([random_train_sentence])\n",
        "print(f'sentence after embedding:\\n{use_embedding_sentence[0][:30]} \\n')\n",
        "print(f'Length of sentence embedding: {len(use_embedding_sentence[0])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLW_r5r8X2ti"
      },
      "source": [
        "### Building and fitting NLP ffeature extraction model using pretrained embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 190,
      "metadata": {
        "id": "JbNQRbTtc7KV"
      },
      "outputs": [],
      "source": [
        "# Define feature extraction layer.\n",
        "inputs = layers.Input(shape=[], dtype=tf.string)\n",
        "pretrained_embedding = tf_hub_embedding_layer(inputs)\n",
        "x = layers.Dense(128, activation='relu')(pretrained_embedding)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "model_2 = tf.keras.Model(inputs=inputs,\n",
        "                         outputs=outputs,\n",
        "                         name='model_2_USE_feature_extraction')\n",
        "\n",
        "# Compile the model.\n",
        "model_2.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dxUss9kmesYB"
      },
      "outputs": [],
      "source": [
        "model_2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y85yl3Ewev4H"
      },
      "outputs": [],
      "source": [
        "# Fit the model 2 to the data\n",
        "history_model_2 = model_2.fit(train_dataset,\n",
        "                              epochs=3,\n",
        "                              steps_per_epoch=int(0.1 * len(train_dataset)),\n",
        "                              validation_data=valid_dataset,\n",
        "                              validation_steps=int(0.1 * len(valid_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HR-HQq5kfnG8"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the whole validation dataset\n",
        "model_2.evaluate(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGROvwXsgG9c"
      },
      "outputs": [],
      "source": [
        "# Model 2 predictions\n",
        "model_2_pred_probs = model_2.predict(valid_dataset)\n",
        "model_2_pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yR9Er2jgc68"
      },
      "outputs": [],
      "source": [
        "# Convert the prediction probabilities to labels.\n",
        "model_2_preds = tf.argmax(model_2_pred_probs, axis=1)\n",
        "model_2_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIBQZpQrg6ov"
      },
      "outputs": [],
      "source": [
        "# Culculate results from on the validation dataset.\n",
        "model_2_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                    y_pred=model_2_preds)\n",
        "model_2_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5oUQ2UPhY68"
      },
      "source": [
        "## Model 3: Conv1D with charcter embedding\n",
        "\n",
        "In the reaserch paper they used both charactor and token level embedding\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wq8-Bv0ZkOZ7"
      },
      "source": [
        "### Creating a character-level tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAKxclQPkVZh"
      },
      "outputs": [],
      "source": [
        "# Make fuctions to spit sentences into characters.\n",
        "def split_chars(text):\n",
        "  return ' '.join(list(text))\n",
        "\n",
        "# Test the split\n",
        "split_chars(random_train_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9guVB965kZjU"
      },
      "outputs": [],
      "source": [
        " # Split sequence-level data splits into character-level data splits\n",
        "train_chars = [split_chars(sentence) for sentence in train_sentences]\n",
        "val_chars = [split_chars(sentence) for sentence in val_sentences]\n",
        "test_chars = [split_chars(sentence) for sentence in test_sentences]\n",
        "train_chars[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 199,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YyBcQIjCmCRC",
        "outputId": "7138f2b4-a2a5-4e70-f38a-e3d11792dca3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "149.3662574983337"
            ]
          },
          "metadata": {},
          "execution_count": 199
        }
      ],
      "source": [
        "# What is the average characer length?\n",
        "char_lens = [len(sentence) for sentence in train_sentences]\n",
        "mean_char_len = np.mean(char_lens)\n",
        "mean_char_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ihh-BtMMo_-0"
      },
      "outputs": [],
      "source": [
        "# Check the distribution of the sequences at character-level\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(char_lens, bins=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 201,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EHpAGv7pptnY",
        "outputId": "6010c684-4e9c-4c15-81f4-f006cb1ada88"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "290"
            ]
          },
          "metadata": {},
          "execution_count": 201
        }
      ],
      "source": [
        "# Find what character length coves 95% of sequences\n",
        "output_seq_char_len = int(np.percentile(char_lens, 95))\n",
        "output_seq_char_len"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 202,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "z6wMpbUCqURS",
        "outputId": "0e31beff-08c3-43c4-f855-c927019c6b0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'abcdefghijklmnopqrstuvwxyz0123456789!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 202
        }
      ],
      "source": [
        "# Get all keyboard characters\n",
        "import string\n",
        "alphabet = string.ascii_lowercase + string.digits + string.punctuation\n",
        "alphabet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 203,
      "metadata": {
        "id": "bTrqDOkTrz_6"
      },
      "outputs": [],
      "source": [
        "# Create a char-level token vectorizer instance\n",
        "NUM_CHAR_TOKENS = len(alphabet) + 2\n",
        "char_vectorizer = TextVectorization(max_tokens=NUM_CHAR_TOKENS,\n",
        "                                    output_sequence_length=output_seq_char_len,\n",
        "                                    standardize='lower_and_strip_punctuation',\n",
        "                                    name='char_vectorizer')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 204,
      "metadata": {
        "id": "IrGZA_5Mtgfn"
      },
      "outputs": [],
      "source": [
        "# Adapt character vectorizer to training character\n",
        "char_vectorizer.adapt(train_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNSITr3Gt8hE"
      },
      "outputs": [],
      "source": [
        "# Check char vocab start\n",
        "char_vocab = char_vectorizer.get_vocabulary()\n",
        "print(f\"number of diff chars: {len(char_vocab)}\")\n",
        "print(f'5 most common char: {char_vocab[:5]}')\n",
        "print(f'5 least common char: {char_vocab[-5:]}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DLtqOp-xusGv"
      },
      "outputs": [],
      "source": [
        "# Test out character vectorizer\n",
        "random_train_chars = random.choice(train_chars)\n",
        "print(f\"charified text:\\n {random_train_chars}\")\n",
        "print(f\"Length of random_train_chars: {len(random_train_chars.split())}\")\n",
        "vectorized_chars = char_vectorizer([random_train_chars])\n",
        "print(f'\\nvectorized chars: \\n{vectorized_chars}')\n",
        "print(f'\\n Length of the vectorized chars: {len(vectorized_chars[0])}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPUeGPh-wuik"
      },
      "source": [
        "### Creating char embedding layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 207,
      "metadata": {
        "id": "XnIc4T20M-m6"
      },
      "outputs": [],
      "source": [
        "char_embed = layers.Embedding(input_dim=len(char_vocab),\n",
        "                              output_dim=25,\n",
        "                              mask_zero=True,\n",
        "                              name=\"char_embed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RK9b5s1eN4Mc"
      },
      "outputs": [],
      "source": [
        "# Test character embedding layer.\n",
        "print(f\"Charified text:\\n {random_train_chars} \\n\")\n",
        "char_embed_example = char_embed(char_vectorizer([random_train_chars]))\n",
        "print(f\"Embedded chars. \\n {char_embed_example}\")\n",
        "print(f'Character embedding shape: {char_embed_example.shape}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3Ul3xpQO9Ot"
      },
      "source": [
        "### CHarector level embedding model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 209,
      "metadata": {
        "id": "ovi81ejxTnke"
      },
      "outputs": [],
      "source": [
        "inputs = layers.Input(shape=(1,), dtype='string')\n",
        "char_vectors = char_vectorizer(inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "x = layers.Conv1D(64, kernel_size=5, padding='same', activation='relu')(char_embeddings)\n",
        "x = layers.GlobalMaxPool1D()(x)\n",
        "outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "model_3 = tf.keras.Model(inputs=inputs,\n",
        "                         outputs=outputs,\n",
        "                         name='model_3_conv1d_char_embeddings')\n",
        "\n",
        "# Compile the model.\n",
        "model_3.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IqMQDCrrVai4"
      },
      "outputs": [],
      "source": [
        "model_3.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7aBeJb4Vdg4"
      },
      "outputs": [],
      "source": [
        "# Create character level datasets\n",
        "train_char_dataset = tf.data.Dataset.from_tensor_slices((train_chars, train_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "val_char_dataset = tf.data.Dataset.from_tensor_slices((val_chars, val_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "test_char_dataset = tf.data.Dataset.from_tensor_slices((test_chars, test_labels_one_hot)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "train_char_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c3nO0gWW-cm"
      },
      "outputs": [],
      "source": [
        "# Fit the model on chars only\n",
        "model_3_history = model_3.fit(train_char_dataset,\n",
        "                              steps_per_epoch=int(0.1*len(train_char_dataset)),\n",
        "                              epochs=3,\n",
        "                              validation_data=val_char_dataset,\n",
        "                              validation_steps=int(0.1*len(val_char_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fw3OrOmnXu6O"
      },
      "outputs": [],
      "source": [
        "# Make predictions with charaster models\n",
        "model_3_pred_probs = model_3.predict(val_char_dataset)\n",
        "model_3_pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 214,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8f81RKYbYy3k",
        "outputId": "56ef7303-7376-4946-c00a-9e1f086179f7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([1, 1, 2, ..., 4, 4, 0])>"
            ]
          },
          "metadata": {},
          "execution_count": 214
        }
      ],
      "source": [
        "# Conver the prediction probabilities to class label's\n",
        "model_3_preds = tf.argmax(model_3_pred_probs, axis=1)\n",
        "model_3_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JiCMdz8UZn2P"
      },
      "outputs": [],
      "source": [
        "# Calc the results of model 3\n",
        "model_3_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                    y_pred=model_3_preds)\n",
        "model_3_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ta9ne48KZ9gA"
      },
      "source": [
        " ## Model 4: Combining pretrained token embedding and the charactor embedding (hybrid embedding layer)\n",
        "\n",
        "1. Create a token-level embedding model (similar `model_1`)\n",
        "2. Create a character-level model (similar to `model_3` with a slight modification)\n",
        "3. Combine 1 & 2 with a concatenate (`layers.Concatenate`)\n",
        "4. Build a series of output layers on top of 3 similar to Figure 1 and section 4.2 of the paper\n",
        "5. Construct a model that takes token and character-level sequences as inputs and produce a sequence labels probabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 216,
      "metadata": {
        "id": "0vbFly6blILp"
      },
      "outputs": [],
      "source": [
        "# 1. Setup token inputs.model\n",
        "token_inputs = layers.Input(shape=[], dtype=tf.string, name='token_input')\n",
        "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
        "token_outputs = layers.Dense(128, activation='relu')(token_embeddings)\n",
        "token_model = tf.keras.Model(inputs=token_inputs,\n",
        "                             outputs=token_outputs)\n",
        "\n",
        "# 2. Setup char inputs/model\n",
        "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name='char_input')\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(24)) (char_embeddings)\n",
        "char_model = tf.keras.Model(inputs=char_inputs,\n",
        "                            outputs=char_bi_lstm)\n",
        "\n",
        "# 3. Concatenate token and char inputs (create hybrid token embeddin)\n",
        "token_char_concat = layers.Concatenate(name=\"token_char_hybrid\")([token_model.output,\n",
        "                                                                  char_model.output])\n",
        "\n",
        "# 4. Create an output layer - adding in Dropout\n",
        "combined_dropout = layers.Dropout(0.5)(token_char_concat)\n",
        "combined_dense = layers.Dense(128, activation='relu')(combined_dropout)\n",
        "final_dropout = layers.Dropout(0.5)(combined_dense)\n",
        "output_layer = layers.Dense(num_classes, activation='softmax')(final_dropout)\n",
        "\n",
        "# 5. construct model with char and token inputs\n",
        "model_4 = tf.keras.Model(inputs=[token_model.input, char_model.input],\n",
        "                      outputs=output_layer,\n",
        "                      name='model_4_token_and_char'\n",
        "                      )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCXvwdOou938"
      },
      "outputs": [],
      "source": [
        "model_4.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G0inpfspwwId"
      },
      "outputs": [],
      "source": [
        "# Plot the hybrid token and character model\n",
        "from keras.utils import plot_model\n",
        "plot_model(model_4, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 219,
      "metadata": {
        "id": "fxqYLXHvyHC9"
      },
      "outputs": [],
      "source": [
        "# Compile token char model\n",
        "model_4.compile(loss='categorical_crossentropy',\n",
        "                optimizer=tf.keras.optimizers.Adam(),  # you can as well use SGD\n",
        "                metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqEVPFXb0p3Z"
      },
      "source": [
        "### Combining token and sharacter data into a tf.data.Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 220,
      "metadata": {
        "id": "h85Rh6QCUOpS"
      },
      "outputs": [],
      "source": [
        "# Combinr chars and tokens into a dataset.\n",
        "train_char_token_data = tf.data.Dataset.from_tensor_slices((train_sentences, train_chars)) # make data.\n",
        "train_char_token_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)   # make labels.\n",
        "train_char_token_dataset = tf.data.Dataset.zip((train_char_token_data, train_char_token_labels))  # Combines the data and labels\n",
        "\n",
        "# Prefetch and batch train data.\n",
        "train_char_token_dataset = train_char_token_dataset.batch(23).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 221,
      "metadata": {
        "id": "ydA3WSM1XBiD"
      },
      "outputs": [],
      "source": [
        "# Repeat the process for the validation dataset.\n",
        "val_char_token_data = tf.data.Dataset.from_tensor_slices((val_sentences, val_chars))\n",
        "val_char_token_label = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_char_token_dataset = tf.data.Dataset.zip((val_char_token_data, val_char_token_label))\n",
        "\n",
        "val_char_token_dataset = val_char_token_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPLxaOA5YRcI"
      },
      "outputs": [],
      "source": [
        "# Chesk the trainin char and token embeddin dataset.\n",
        "train_char_token_dataset, val_char_token_dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsGld_ToYkNj"
      },
      "source": [
        "### Fiting a model on token and char level sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OEuq1QqjZar1"
      },
      "outputs": [],
      "source": [
        "# FIt the model on tokena and char datasets.\n",
        "history_model_4 = model_4.fit(train_char_token_dataset,\n",
        "                              steps_per_epoch=int(0.1*len(train_char_token_dataset)),\n",
        "                              epochs=3,\n",
        "                              validation_data=val_char_token_dataset,\n",
        "                              validation_steps=int(0.1 * len(val_char_token_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1ihC_kYaipy"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model.\n",
        "model_4.evaluate(val_char_token_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9RcojQU6bvtY"
      },
      "outputs": [],
      "source": [
        "# Make predictions for model 4 on the validation char token dataset.\n",
        "model_4_pred_probs = model_4.predict(val_char_token_dataset)\n",
        "model_4_pred_probs[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNcVUObNb-zQ"
      },
      "outputs": [],
      "source": [
        "# Format the predection probablities into labels.\n",
        "model_4_preds = tf.argmax(model_4_pred_probs, axis=1)\n",
        "model_4_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c715ZnL1eZIk"
      },
      "outputs": [],
      "source": [
        "# Get results of token-char-hybrid model.\n",
        "model_4_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                   y_pred=model_4_preds)\n",
        "model_4_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZtH99C0ez9X"
      },
      "source": [
        "## Model 5: Transfer learnin with pretrained token embeddings + character embeddings + positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEf03j_diQs5"
      },
      "outputs": [],
      "source": [
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2JjsPJWi4sx"
      },
      "source": [
        "**NOTE:** Any eningineered feature that was used at train time need to be availabel at test time.\n",
        "\n",
        "Like the line numbers and total lines are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQGdFMWQmgnc"
      },
      "outputs": [],
      "source": [
        "# How many different line numbers are there?\n",
        "train_df['line_number'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c2-msH8sn2Uz"
      },
      "outputs": [],
      "source": [
        "# Find the distribution of the line number\n",
        "train_df.line_number.plot.hist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_saMVGcoFas"
      },
      "outputs": [],
      "source": [
        "# Create one-hot-encoded tensors using tensorflow for the line number in the dataset.\n",
        "train_line_numbers_one_hot = tf.one_hot(train_df['line_number'].to_numpy(), depth=15)\n",
        "val_line_number_one_hot = tf.one_hot(val_df['line_number'].to_numpy(), depth=15)\n",
        "test_line_number_one_hot = tf.one_hot(test_df['line_number'].to_numpy(), depth=15)\n",
        "train_line_numbers_one_hot[:10], train_line_numbers_one_hot.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4Ti1mulpACZ"
      },
      "outputs": [],
      "source": [
        "# One hot encode the total numbers of lines in the dataset.\n",
        "train_df['total_lines'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d26MIK-qqoEP"
      },
      "outputs": [],
      "source": [
        "# Check the coverae of a total_line value of 20\n",
        "np.percentile(train_df.total_lines, 98)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uW7ER2pUsQRJ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iTKeJ1rorv0d"
      },
      "outputs": [],
      "source": [
        "train_total_lines_one_hot = tf.one_hot(train_df['total_lines'].to_numpy(), depth=20)\n",
        "val_total_lines_one_hot = tf.one_hot(val_df['total_lines'].to_numpy(), depth=20)\n",
        "test_total_lines_one_hot = tf.one_hot(test_df['total_lines'].to_numpy(), depth=20)\n",
        "train_total_lines_one_hot[:10], train_total_lines_one_hot.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yawb8yD2sOIW"
      },
      "source": [
        "### Building a tribrid embeddin model\n",
        "\n",
        "1. Create a token-level model\n",
        "2. Create a character-level model.\n",
        "3. Create a model for the 'line_number' features.\n",
        "4. Create a model for the 'total_lines' features.\n",
        "5. Combine the outputs of one and two usin tf.keras.layers.Concatenate\n",
        "6. Combine the outputs of 3, 4, 5 using tf.keras.layers.Concatinate.\n",
        "7. Create an output layer to accept the tribried embedding and output label probabilities\n",
        "8. Combine the inputs of 1, 2, 3, 4 and the outputs of 7 into tf.keras.Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 249,
      "metadata": {
        "id": "AEdkUyIDuMGw"
      },
      "outputs": [],
      "source": [
        "# Token inputs\n",
        "token_inputs = layers.Input(shape=[], dtype=tf.string, name='token_inputs')\n",
        "token_embeddings = tf_hub_embedding_layer(token_inputs)\n",
        "token_outputs = layers.Dense(128, activation='relu')(token_embeddings)\n",
        "token_model = tf.keras.Model(inputs=token_inputs,\n",
        "                             outputs=token_outputs)\n",
        "\n",
        "# Char inputs\n",
        "char_inputs = layers.Input(shape=(1,), dtype=tf.string, name='char_input')\n",
        "char_vectors = char_vectorizer(char_inputs)\n",
        "char_embeddings = char_embed(char_vectors)\n",
        "char_bi_lstm = layers.Bidirectional(layers.LSTM(24)) (char_embeddings)\n",
        "char_model = tf.keras.Model(inputs=char_inputs,\n",
        "                            outputs=char_bi_lstm)\n",
        "\n",
        "# Line mubers model\n",
        "line_number_inputs = layers.Input(shape=(15,), dtype=tf.float32, name='line_number_input')\n",
        "x = layers.Dense(32, activation='relu')(line_number_inputs)\n",
        "line_number_model = tf.keras.Model(inputs=line_number_inputs,\n",
        "                                   outputs=x)\n",
        "\n",
        "# Total lines model.\n",
        "total_lines_inputs = layers.Input(shape=(20,), dtype=tf.float32, name='total_kines_input')\n",
        "y = layers.Dense(32, activation='relu')(total_lines_inputs)\n",
        "total_lines_model = tf.keras.Model(inputs=total_lines_inputs,\n",
        "                                  outputs=y)\n",
        "\n",
        "# Combine the outputs of steps one and two by pass their outputs as a list.\n",
        "combined_embeddings = layers.Concatenate(name='char_token_hybrid_embedding')([token_model.output,\n",
        "                                                                             char_model.output])\n",
        "\n",
        "z = layers.Dense(256, activation='relu')(combined_embeddings)\n",
        "z = layers.Dropout(0.5)(z)\n",
        "\n",
        "# Combine positional embedding with combined token and char embeddings\n",
        "tribrid_embeddings = layers.Concatenate(name='char_token_positional_embedding')([line_number_model.output,\n",
        "                                                                                 total_lines_model.output,\n",
        "                                                                                 z])\n",
        "# Create output layer.\n",
        "output_layer = layers.Dense(5, activation='softmax', name='output_layer')(tribrid_embeddings)\n",
        "\n",
        "# Finaly put it all together\n",
        "model_5 = tf.keras.Model(inputs=[line_number_model.input,\n",
        "                                 total_lines_model.input,\n",
        "                                 token_model.input,\n",
        "                                 char_model.input],\n",
        "                         outputs=output_layer,\n",
        "                         name='tribrid_embedding')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qa8lUkyq2vMM"
      },
      "outputs": [],
      "source": [
        "# Get the summary for the model.\n",
        "model_5.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Y2DzkWx_J9_"
      },
      "outputs": [],
      "source": [
        "# Plot model_5 to explore it visualy\n",
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model_5, show_shapes=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "CcPAvOu7_fOR"
      },
      "outputs": [],
      "source": [
        "# Compile the token, char, and positional embeddin model.\n",
        "# Label smoothing to prevent overfitting by improving generalization\n",
        "model_5.compile(loss=tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.2),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "86cVqPCtCRr7"
      },
      "source": [
        "### Create tribrid embedding data using tf.data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 246,
      "metadata": {
        "id": "ClG7kcBNDV0l"
      },
      "outputs": [],
      "source": [
        "# Create training and validation dataset (with all four kinds of input data)\n",
        "\n",
        "train_char_token_pos_data = tf.data.Dataset.from_tensor_slices((train_line_numbers_one_hot,\n",
        "                                                               train_total_lines_one_hot,\n",
        "                                                               train_sentences,\n",
        "                                                               train_chars))\n",
        "train_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(train_labels_one_hot)\n",
        "train_char_token_pos_dataset = tf.data.Dataset.zip((train_char_token_pos_data, train_char_token_pos_labels))\n",
        "train_char_token_pos_dataset = train_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "# for the validation dataset.\n",
        "val_char_token_pos_data = tf.data.Dataset.from_tensor_slices((val_line_number_one_hot,\n",
        "                                                             val_total_lines_one_hot,\n",
        "                                                             val_sentences,\n",
        "                                                             val_chars))\n",
        "val_char_token_pos_labels = tf.data.Dataset.from_tensor_slices(val_labels_one_hot)\n",
        "val_char_token_pos_dataset = tf.data.Dataset.zip((val_char_token_pos_data, val_char_token_pos_labels))\n",
        "val_char_token_pos_dataset = val_char_token_pos_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-LbFCcvbkDiN"
      },
      "execution_count": 239,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Bbnr38ZGktu"
      },
      "outputs": [],
      "source": [
        "# Check input shapes\n",
        "train_char_token_pos_dataset, val_char_token_pos_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RbCBmcIIbuu"
      },
      "outputs": [],
      "source": [
        "# Fit the model.\n",
        "hitory_model_5 = model_5.fit(train_char_token_pos_dataset,\n",
        "                             steps_per_epoch=int(0.1 * len(train_char_token_pos_dataset)),\n",
        "                             epochs=5,\n",
        "                             validation_data=val_char_token_pos_dataset,\n",
        "                             validation_steps=int(0.1*len(val_char_token_pos_dataset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "acB7geOAKryQ"
      },
      "outputs": [],
      "source": [
        "# Make predicions with the char token pos model.\n",
        "model_5_pred_probs = model_5.predict(val_char_token_pos_dataset, verbose=1)\n",
        "model_5_pred_probs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 260,
      "metadata": {
        "id": "DXj45eH9Lh0C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b13a1935-ed3c-4ccf-b7d7-a2fec70a23e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(30212,), dtype=int64, numpy=array([0, 0, 3, ..., 4, 4, 1])>"
            ]
          },
          "metadata": {},
          "execution_count": 260
        }
      ],
      "source": [
        "# Convert pred probs to pred labels'\n",
        "model_5_preds = tf.argmax(model_5_pred_probs, axis=1)\n",
        "model_5_preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBxXXOgwL0ds"
      },
      "outputs": [],
      "source": [
        "# Calculate the results of model_5\n",
        "model_5_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                   y_pred=model_5_preds)\n",
        "model_5_results"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Compare the models results.\n",
        "all_model_results = pd.DataFrame({\"model_0_baseline\": baseline_results,\n",
        "                                  \"model_1_custom_token_embedding\": model_1_results,\n",
        "                                  \"model_2_pretrained_token_embeddin\": model_2_results,\n",
        "                                  \"model_3_custom_char_embedding\": model_3_results,\n",
        "                                  \"model_4_hybrid_char_token_embedding\": model_4_results,\n",
        "                                  \"model_5_pos_char_token_embedding\": model_5_results,\n",
        "                                 })"
      ],
      "metadata": {
        "id": "TJ0mfhv9-ZD0"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_model_results = all_model_results.transpose()\n",
        "all_model_results"
      ],
      "metadata": {
        "id": "VQ9EIk3CDl1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_model_results.accuracy = all_model_results['accuracy']/100"
      ],
      "metadata": {
        "id": "6BbmPuvUDy_N"
      },
      "execution_count": 275,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_model_results.plot(kind='bar', figsize=(10, 7)).legend(bbox_to_anchor=(1.0, 1.0))"
      ],
      "metadata": {
        "id": "Q-yaEc3-E6PI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the results by f1 score\n",
        "all_model_results.sort_values('f1', ascending=True)['f1'].plot(kind='bar', figsize=(10, 7))"
      ],
      "metadata": {
        "id": "kLzp_eVyFwiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Save and load model"
      ],
      "metadata": {
        "id": "3BVso2D1Gbui"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_5.save(\"skimlit_tribrid_model\")"
      ],
      "metadata": {
        "id": "M9KeW9HoHPeE"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model = tf.keras.models.load_model(\"skimlit_tribrid_model\")"
      ],
      "metadata": {
        "id": "Ca_92HkuHZA3"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_pred_probs = loaded_model.predict(val_char_token_pos_dataset)\n",
        "loaded_preds = tf.argmax(loaded_model_pred_probs, axis=1)\n",
        "loaded_preds[:10]"
      ],
      "metadata": {
        "id": "T_G1T54VH84H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loaded_model_results = calculate_results(y_true=val_labels_encoded,\n",
        "                                         y_pred=loaded_preds)\n",
        "loaded_model_results"
      ],
      "metadata": {
        "id": "9mf5Y4EkJayV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "assert model_5_results == loaded_model_results"
      ],
      "metadata": {
        "id": "5zl79GscJvs2"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G2NdAOqtJ1CZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyMqAAc2l+w9zfPV0jpawFVJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}